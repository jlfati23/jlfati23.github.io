---
title: "Java Multhreading and Concurrency"
date: 2024-04-09
tags:
- multhreading
- concurrency
---

## Threading Fundamentals

```java
public class ThreadingFundamentals {

    public static void main(String[] args) {
        // 1. Creating Threads
        // 1.1. Extending the Thread class
        Thread thread1 = new MyThread("Thread 1");

        // 1.2. Implementing the Runnable interface
        Thread thread2 = new Thread(new MyRunnable(), "Thread 2");

        // 2. Starting Threads
        thread1.start();
        thread2.start();

        // 3. Main Thread
        System.out.println("Main thread is running...");

        // 4. Thread States
        // Thread.State state1 = thread1.getState(); // Example usage

        // 5. Joining Threads
        try {
            thread1.join(); // Wait for thread1 to finish
            thread2.join(); // Wait for thread2 to finish
        } catch (InterruptedException e) {
            System.err.println("Thread interrupted: " + e.getMessage());
        }

        System.out.println("Main thread finished.");
    }
}

// 1.1. Extending the Thread class
class MyThread extends Thread {
    public MyThread(String name) {
        super(name);
    }

    @Override
    public void run() {
        System.out.println(getName() + " is running...");
        // Perform some task
    }
}

// 1.2. Implementing the Runnable interface
class MyRunnable implements Runnable {
    @Override
    public void run() {
        System.out.println(Thread.currentThread().getName() + " is running...");
        // Perform some task
    }
}
```

**Explanation:**

1. **Creating Threads:**
    - **1.1. Extending the `Thread` class:**
        - Create a class that extends the `java.lang.Thread` class.
        - Override the `run()` method to define the thread's task.
        - Create an instance of your custom thread class.
    - **1.2. Implementing the `Runnable` interface:**
        - Create a class that implements the `java.lang.Runnable` interface.
        - Implement the `run()` method to define the thread's task.
        - Create an instance of the `Thread` class, passing an instance of your `Runnable` class to the constructor.

2. **Starting Threads:**
    - Call the `start()` method on the `Thread` object. This creates a new thread of execution and invokes the `run()`
      method.

3. **Main Thread:**
    - The program's execution starts with the main thread.
    - The `main()` method executes in the main thread.

4. **Thread States:**
    - Threads in Java have different states (e.g., NEW, RUNNABLE, BLOCKED, TERMINATED).
    - You can get the current state of a thread using `thread.getState()`.

5. **Joining Threads:**
    - The `join()` method allows one thread to wait for another thread to complete its execution.
    - In the example, the main thread waits for `thread1` and `thread2` to finish before printing "Main thread
      finished."

**Key Concepts:**

- **Thread:** A lightweight, independent unit of execution **within a process.**
- **Concurrency:** Running multiple threads concurrently, potentially improving performance.
- **Thread Safety:** Ensuring that shared resources are accessed and modified by multiple threads in a safe and
  predictable manner.
- **Synchronization:** Mechanisms (e.g., locks, semaphores) to control access to shared resources and prevent race
  conditions.

More advanced concepts include thread pools, synchronization primitives, and concurrent data structures.

## Performance in Multithreaded Applications

Measuring the performance of multithreaded applications can be tricky, as it involves understanding not just how fast
your code executes, but also how effectively it utilizes system resources and handles concurrency.

**1. Execution Time:**

- **Wall-Clock Time:** This is the simplest metric, measuring the total time taken from the start of your multithreaded
  application to its end. You can use tools like `System.currentTimeMillis()` in Java to get timestamps and calculate
  the difference.
  ```java
  long startTime = System.currentTimeMillis();
  // Your multithreaded code here
  long endTime = System.currentTimeMillis();
  long executionTime = endTime - startTime;
  System.out.println("Execution Time: " + executionTime + " ms");
  ```
- **Keep in mind:**  Wall-clock time alone doesn't tell the whole story. A faster time doesn't always mean better
  performance, especially if it comes at the cost of resource utilization or scalability.

**2. Throughput and Latency:**

- **Throughput:**  Measures how much work your application completes per unit of time. For example, transactions
  processed per second, requests handled per minute, etc. Higher throughput generally indicates better performance.
    - **Approaches to improve throughtput**
        - Breaking tasks into subtasks, which has an inherent cost
        - Running tasks in parallel, which is suitable for parallelizable tasks, it will not cost the breaking task and
          aggragating results

- **Latency:**  **Measures the time it takes to complete a single unit of work**. For instance, the time to process a
  single request or execute a task. Lower latency is usually desirable.
    - We can devide our task into **multiple sub tasks**. The number of sub tasks is usually equal to the number of
      cores, so that it can increase the latency by running all of your sub tasks in parallel. However, the
      formula `#sub tasks = # number of cores `is optimal only if all the threads are runnable and can run without
      interruption(no IO/blocking calls or sleep etc), and we also assume that nothing else that comsums a lot of cpu is
      running.
    - We can never run our tasks totally in parallel , but we can get close to it.
    - Inherent cost of parallelization and aggregation
        1. breaking task into sub tasks
        2. Thread creation and pass tasks to thread
        3. time between thread.start() to thread getting scheduled
        4. time units the last thread finishes and signals
        5. time units the aggregating thread runs
        6. aggregation of the subresults into a siggle artifact
    - Can we break any task into subtasks? No
        - Three types of tasks: Parallelizable tasks ; Unbreakable, sequentical tasks; Partially parallelizable,
          partially sequential tasks.

**3. Resource Utilization:**

- **CPU Utilization:** Monitor how effectively your threads utilize available CPU cores. Tools like the Task Manager (
  Windows) or `top` (Linux/macOS) can help. High CPU utilization is good, but consistently maxing out all cores might
  indicate bottlenecks.
- **Memory Usage:** Track your application's memory consumption to identify potential memory leaks or excessive object
  creation. Profiling tools can be invaluable here.
- **I/O Operations:** If your application involves disk or network I/O, measure these operations as they can
  significantly impact performance.

**4. Concurrency Metrics:**

- **Thread Creation/Destruction Time:** Frequent thread creation and destruction can introduce overhead. Consider using
  thread pools to reuse threads.
- **Context Switching:**  The operating system rapidly switches between threads to give the illusion of parallelism.
  Excessive context switching can degrade performance.
- **Synchronization Overhead:**  Locks, semaphores, and other synchronization mechanisms introduce overhead. Profile
  your code to identify contention points and optimize locking strategies.

**Tools and Techniques:**

- **Profiling Tools:** Use profilers like JProfiler, YourKit, or VisualVM (built into JDK) to analyze thread activity,
  method-level execution times, memory allocations, and identify performance bottlenecks.
- **Benchmarking Frameworks:** JMH (Java Microbenchmark Harness) is excellent for writing rigorous microbenchmarks to
  compare different implementations or algorithms.
- **Load Testing:** Simulate realistic workloads using tools like JMeter or Gatling to understand how your application
  performs under stress.

**Remember:**

- Performance optimization is iterative. Measure, identify bottlenecks, optimize, and measure again.
- Avoid premature optimization. Focus on the critical parts of your code that impact performance the most.
- Consider the trade-offs between different optimization techniques. Sometimes, a small performance gain might come at
  the cost of code complexity or maintainability.

## Data Sharing Between Threads

### Stack Memory vs Heap Memory

**1. Stack Memory:**

- **Stack Memory Region is where** (Exlusive)

    - Methods are called
    - Arguments are passed
    - Local variables are stored

- **Structure:** Organized as a LIFO (Last-In, First-Out) stack, similar to a stack of plates.

- **Purpose:**
    - Stores **method call information** (method frames): When a method is called, a new frame is pushed onto the stack
      containing local variables, parameters, return address, and other temporary data. When the method completes, its
      frame is popped off.
    - Enables **function call hierarchy** and recursion.

- **Scope:** Local variables declared within a method exist only on the stack frame for that method's execution. They
  are not directly accessible outside that scope.

- **Thread-Specific:** **Each thread gets its own stack**. This is crucial for concurrency as it isolates threads from
  interfering with each other's method calls and local data.

**2. Heap Memory:**

- **Heap Memory Region is where** (Shared)
    - Objects: String/Object/Collection
    - Members of classes
    - Static variables

- **Structure:** A less organized region for dynamic memory allocation.
- **Purpose:** Stores **objects** (instances of classes) and the data associated with them.
- **Scope:** Objects created on the heap have a lifespan that can extend beyond the methods that created them. They can
  be accessed from different parts of the application as long as a reference to the object exists.
- **Shared Resource:** The heap is the **primary location for shared state** in multithreaded applications. Objects on
  the heap can be referenced by multiple threads. Heap memory region is governed and managed **by Java Garbage Collector
  **, and objects stay as long as we have a reference to them. Members of classed stay as long as their parent objects
  exist(same life cycle as their parents), static variables stay as the application exists.

**Relationship with Shared State and Multithreading:**

- **Stack and Isolation:**  The thread-specific nature of the stack provides inherent isolation for local variables.
  Since each thread has its own stack, local variables are never directly shared between threads. This helps prevent
  many concurrency issues.

- **Heap and Shared Data:** The heap is where shared state lives. When multiple threads access and potentially modify
  the same objects on the heap, you need synchronization mechanisms (locks, synchronized blocks, etc.) to prevent data
  races and ensure thread safety.

**Example:**

```java
public class SharedStateExample {

    private static class Counter {
        private int value = 0;

        public synchronized void increment() {
            value++;
        }

        public int getValue() {
            return value;
        }
    }

    public static void main(String[] args) {
        Counter sharedCounter = new Counter(); // Shared object on the heap

        Thread thread1 = new Thread(() -> {
            for (int i = 0; i < 1000; i++) {
                sharedCounter.increment();
            }
        });

        Thread thread2 = new Thread(() -> {
            for (int i = 0; i < 1000; i++) {
                sharedCounter.increment();
            }
        });

        thread1.start();
        thread2.start();

        try {
            thread1.join();
            thread2.join();
        } catch (InterruptedException e) {
            e.printStackTrace();
        }

        System.out.println("Final value: " + sharedCounter.getValue());
    }
}
```

- **Shared `Counter`:** The `Counter` object is created on the heap, making it accessible to both threads.
- **`increment()` Synchronization:** The `synchronized` keyword on the `increment()` method ensures that only one thread
  can modify the `value` at a time, preventing data corruption.

### Concurrency Chanllenges and Solutions

Data sharing between threads is a fundamental aspect of concurrent programming in Java, allowing threads to cooperate
and exchange information. However, it also introduces the risk of data races and inconsistencies if not handled
carefully.

**1. Shared Data and the Problem:**

- When multiple threads access and potentially modify the same data (variables, objects), you have shared data.
- The problem arises because thread execution is **non-deterministic.** If one thread's changes to shared data are not
  properly synchronized, other threads might see an inconsistent or outdated view of that data, leading to errors.

**2. Synchronization Mechanisms:**

- **Locks (`java.util.concurrent.locks.Lock`):**

    - Provide exclusive access to a shared resource. Only one thread can hold the lock at a time.
    - Use `lock.lock()` to acquire the lock and `lock.unlock()` to release it (ideally in a `finally` block to guarantee
      release).

  ```java
  private Lock lock = new ReentrantLock();
  private int sharedCounter = 0;
  
  public void incrementCounter() {
      lock.lock(); 
      try {
          sharedCounter++;
      } finally {
          lock.unlock(); 
      }
  }
  ```

- **Synchronized Blocks/Methods (coarse-grained strategy):**

    - A more concise way to achieve synchronization.
    - A `synchronized` block uses the object's intrinsic lock (or the class lock for static methods).

  ```java
  private int sharedCounter = 0;
  
  public synchronized void incrementCounter() {
      sharedCounter++; 
  }
  ```

- **Volatile Keyword:**
    - Ensures that changes to a variable are immediately visible to all threads.
    - Use with caution – it only guarantees visibility, not atomicity (for compound operations like increment).

  ```java
  private volatile boolean flag = false;
  
  // In one thread:
  flag = true;
  
  // In another thread:
  if (flag) { 
      // ...
  }
  ```

**3. Concurrent Data Structures:**

- Java provides thread-safe data structures in the `java.util.concurrent` package, designed for concurrent access
  without the need for explicit locking:
    - `ConcurrentHashMap`: A thread-safe version of `HashMap`.
    - `ConcurrentLinkedQueue`: A thread-safe, non-blocking queue.
    - `BlockingQueue`: An interface for queues with blocking operations (e.g., `put()`, `take()`).
    - `AtomicInteger`, `AtomicLong`, etc.: Classes for atomic operations on primitive types.

**4. Immutable Objects:**

- Immutable objects are inherently thread-safe because their state cannot be changed after creation.
- If data sharing is required, create new immutable objects with the updated values.

**5. Thread-Local Variables (`ThreadLocal`):**

- Provide a separate copy of a variable for each thread that accesses it.
- Useful when you need per-thread data isolation.

  ```java
  private static ThreadLocal<SimpleDateFormat> dateFormat = 
      ThreadLocal.withInitial(() -> new SimpleDateFormat("yyyy-MM-dd"));
  
  public String formatDate(Date date) {
      return dateFormat.get().format(date);
  }
  ```

**Important Considerations:**

- **Deadlocks:**  Avoid circular dependencies between locks, where two or more threads are blocked indefinitely, each
  waiting for the other to release a lock.
- **Liveness:** Ensure your synchronization mechanisms don't introduce liveness issues (e.g., starvation, where a thread
  is perpetually denied access to a resource).
- **Performance:**  While synchronization is essential for correctness, excessive locking can lead to contention and
  performance degradation. Strive for efficient synchronization strategies.

**Choosing the Right Approach:**

- The best approach depends on the specific data sharing scenario.
- For simple operations on shared primitives, `volatile` or atomic classes might suffice.
- For more complex data structures or operations, use locks or synchronized blocks.
- Whenever possible, favor concurrent data structures and immutable objects to simplify concurrency management.

## Race Condition vs Data Race

### Differences

While the terms "race condition" and "data race" are often used interchangeably, there's a subtle but important
distinction between them in the context of concurrent programming:

**1. Race Condition:**

- **Broader Definition:** A race condition is a general flaw in the logic of a program where the output or behavior of
  the program depends on the unpredictable timing of events or the order of execution of multiple threads.
- **Not Limited to Shared Data:**  Race conditions can occur even without shared data.
- **Example:**
    - Imagine two threads trying to write to the same file. A race condition could arise if the program's logic doesn't
      handle the order in which the threads acquire file locks or write data, potentially leading to corrupted data.

**2. Data Race:**

- **Specific to Shared Data:** A data race is a *specific type* of race condition that occurs **exclusively when
  multiple threads access shared data concurrently**, and at least one of those threads is modifying the data.
- **Requires:**
    - At least two threads.
    - Shared data that is both read and written by at least one of the threads.
    - No proper synchronization mechanisms (locks, atomic operations, etc.) to control access to the shared data.
- **Example:**
    - Two threads trying to increment a shared counter without any synchronization. The final value of the counter will
      be incorrect because the threads might read, increment, and write back the value simultaneously, leading to lost
      updates.

**In Summary:**

- **Data race:** A specific type of race condition that *always* involves shared data and concurrent access without
  proper synchronization.
- **Race condition:** A broader category of concurrency bugs that can occur due to various timing or ordering issues,
  including (but not limited to) data races.

**Why the Distinction Matters:**

- **Understanding the root cause:**  Distinguishing between the two helps in pinpointing the root cause of concurrency
  bugs.
- **Applying the right solution:** Data races are addressed by proper synchronization techniques (locks, atomic
  operations). General race conditions might require broader design changes or more complex synchronization strategies.

### Typical Solutions for Addressing Data Races and the Broader Category of Race Conditions

**Solutions for Data Races (Focus on Shared Data):**

The core principle is to ensure that when multiple threads access shared data, they do so in a controlled and
synchronized manner.

1. **Locking:**

    - **Concept:**  Locks provide exclusive access to a shared resource. Only one thread can hold the lock at a time,
      preventing other threads from accessing or modifying the protected data concurrently.
    - **Types:**
        - `ReentrantLock`:  Flexible, explicit locking with options for fairness and timeouts.
        - `synchronized` blocks/methods:  Concise syntax, uses intrinsic object locks.

2. **Atomic Operations:**

    - **Concept:**  Atomic operations provide indivisible, uninterruptible operations on shared data, ensuring that the
      entire operation completes as a single atomic unit.
    - **Classes:**  `AtomicInteger`, `AtomicLong`, `AtomicBoolean`, `AtomicReference`, etc.

3. **Concurrent Data Structures:**

    - **Concept:** These data structures are designed for thread-safety and handle concurrency internally, often using
      sophisticated locking and non-blocking algorithms.
    - **Examples:**  `ConcurrentHashMap`, `ConcurrentLinkedQueue`, `BlockingQueue`, etc.

**Solutions for Broader Race Conditions (Beyond Shared Data):**

1. **Thread Synchronization (Beyond Data):**
    - **Semaphores:** Control access to a limited number of resources (e.g., limiting the number of threads accessing a
      database connection pool).
    - **Barriers (`CyclicBarrier`, `Phaser`):**  Synchronize threads to wait at a common point before proceeding.
    - **Condition Objects (`java.util.concurrent.locks.Condition`):** Allow threads to signal and wait for specific
      conditions related to shared state.

2. **Design Patterns:**
    - **Thread Confinement:** Limit data access to a single thread (e.g., using thread-local variables or
      single-threaded execution models).
    - **Immutable Objects:**  Immutable objects cannot be modified after creation, inherently eliminating data races.
    - **Producer-Consumer Pattern:**  Use queues to safely exchange data between threads, decoupling production and
      consumption.

3. **Careful Code Review and Testing:**

    - **Code Analysis:** Use static analysis tools to detect potential race conditions.
    - **Concurrency Testing:** Employ rigorous testing strategies (e.g., stress testing, concurrency testing tools) to
      identify and reproduce race conditions.

**Choosing the Right Approach:**

- **Data Races:** Prioritize locks, atomic operations, or concurrent data structures for efficient and straightforward
  synchronization of shared data.
- **Complex Race Conditions:** Consider thread synchronization mechanisms, design patterns, or a combination of
  approaches to address broader timing and ordering issues.

**Important Considerations:**

- **Over-synchronization:** Excessive locking can lead to performance bottlenecks and deadlocks. Strive for the right
  balance between synchronization and performance.
- **Complexity:**  Some solutions (e.g., complex locking strategies) can increase code complexity and make debugging
  more challenging.

## Locking Strategy and Deadlock

Locking strategies including coarse-grained locking and fine-grained locking, are aimed to solve concurrent problems,
like race condition and data race, which are innevitable in multithreading programs. The locking strategy itself will
cause Deadlock.

### Conditions For Deadlock

Four classic conditions that **must *all* be present** for a deadlock to occur, often referred to as the **"Coffman
Conditions"** or the **"Deadly Embrace"**:

1. **Mutual Exclusion:**

    - **Explanation:**  A resource (like a lock, a file, or a database connection) can be held by only one thread at a
      time. No two threads can have exclusive access to the same resource simultaneously.
    - **Why Necessary for Deadlock:**  If multiple threads could access a resource concurrently, there would be no
      contention or waiting, and hence, no deadlock.

2. **Hold and Wait:**

    - **Explanation:** A thread is currently holding at least one resource and is waiting to acquire additional
      resources that are being held by other threads.
    - **Why Necessary for Deadlock:** If a thread released its held resources while waiting for new ones, other threads
      could acquire those resources, potentially breaking the deadlock cycle.

3. **No Preemption:**

    - **Explanation:** A resource cannot be forcibly taken away from a thread. A thread can only release a resource
      voluntarily when it has finished using it.
    - **Why Necessary for Deadlock:** If the system could preempt (forcefully release) resources from threads, it could
      break a deadlock by taking a resource from one thread and giving it to another.

4. **Circular Wait:**

    - **Explanation:** There is a circular chain of two or more threads where each thread is waiting for a resource that
      is being held by the next thread in the chain.
    - **Why Necessary for Deadlock:**  A circular dependency is the core of a deadlock. Without it, there might be
      waiting, but one thread would eventually be able to acquire the necessary resources and proceed.

**Analogy:**

Imagine four people at a table, each with one chopstick. They need two chopsticks to eat. If each person refuses to put
down their chopstick and waits indefinitely for someone else to give them another, they'll be stuck in a deadlock –
unable to eat because of the circular dependency.

### Advanced Locks

1. ReentrantLock

2. ReentrantReadWriteLock

3. Semaphore

    - differences with locks : no ownership

    - Inter-thread communication

    - https://pages.cs.wisc.edu/~bart/537/lecturenotes/s6.html

4. Producer and Consumer

5. Condition Variables

6. Objects as condition variables

## Lock-Free Techniques

### Trade-off of Using Locks

The trade-off between synchronization (like locks) and performance. While locks are essential for preventing data races
and ensuring correctness, they do come with costs.

**1. Performance Overhead:**

- **Context Switching:** When a thread tries to acquire a lock that's already held by another thread, it may be
  suspended (descheduled) by the operating system. This context switching between threads has overhead, as the OS needs
  to save and restore the state of the threads involved.
- **Lock Acquisition and Release:** The operations of acquiring and releasing a lock themselves involve some
  computational overhead.
- **Contention:** When multiple threads frequently compete for the same lock (high contention), threads spend more time
  waiting for the lock to become available, reducing overall throughput and increasing latency.

**2. Increased Code Complexity:**

- **Correct Locking:** Using locks correctly to avoid deadlocks and other concurrency bugs requires careful design and
  implementation.
- **Debugging:** Debugging concurrency issues related to locks can be more challenging than debugging single-threaded
  code.

**3. Potential for Deadlocks:**

- **Circular Dependencies:** As discussed earlier, if locks are acquired in an inconsistent order or without proper
  care, you can introduce the risk of deadlocks, where threads are blocked indefinitely, waiting for each other.

**4. Starvation:**

- **Unfair Scheduling:** In some scenarios, certain threads might repeatedly lose lock acquisition to other threads,
  leading to starvation, where a thread is perpetually denied access to a shared resource.

**Mitigating the Costs:**

1. **Fine-Grained Locking:** Use multiple locks to protect smaller, independent data sections, reducing contention.
2. **Lock-Free Data Structures:**  Consider non-blocking algorithms and data structures (e.g., `ConcurrentHashMap`) that
   minimize or eliminate the need for locks.
3. **Optimistic Concurrency:** Use techniques like optimistic locking or stamped locks that allow for lock-free reads in
   certain scenarios.
4. **Thread Confinement:** When possible, confine data access to a single thread, eliminating the need for shared data
   and locks.
5. **Profiling and Performance Tuning:** Use profiling tools to identify performance bottlenecks caused by lock
   contention and optimize your locking strategy accordingly.

**The Trade-off:**

- **Correctness vs. Performance:**  The decision to use locks always involves a trade-off. Locks are crucial for
  correctness in concurrent programs, but they introduce overhead.
- **Finding the Balance:** The goal is to find the right balance between synchronization and performance by:
    - Using locks only when necessary.
    - Minimizing the duration for which locks are held.
    - Choosing appropriate locking strategies to reduce contention.

Lock-free strategies aim to achieve thread safety and synchronization **without relying on traditional locking
mechanisms** like mutexes or semaphores. They offer the potential for increased performance, especially in highly
concurrent systems, but come with increased complexity.

**1. Atomic Operations:**

- **Concept:**  Atomic operations provide indivisible, uninterruptible instructions that execute as a single unit, even
  in multi-threaded environments.
- **How They Work:**  Modern CPUs provide special instructions (e.g., compare-and-swap, fetch-and-add) that guarantee
  atomicity.
- **Java Support:**  The `java.util.concurrent.atomic` package provides classes
  like `AtomicInteger`, `AtomicLong`, `AtomicBoolean`, and `AtomicReference` for atomic operations on primitive types
  and object references.
- **Example:**
  ```java
  private AtomicInteger counter = new AtomicInteger(0);
  
  public int incrementAndGet() {
      return counter.incrementAndGet(); // Atomically increments and returns the new value
  }
  ```

**2. Compare-and-Swap (CAS) Loop:**

- **Concept:**  A fundamental building block for many lock-free algorithms.
- **How It Works:**
    1. Read the current value of a shared variable.
    2. Perform a computation to get a new desired value.
    3. Use a CAS operation to atomically update the variable with the new value, but only if the value hasn't changed
       since it was last read. If the value has changed, the CAS operation fails, and the loop retries.
- **Example:**
  ```java
  private volatile int counter = 0; 
  
  public int increment() {
      int current;
      do {
          current = counter;
      } while (!compareAndSet(current, current + 1)); // CAS loop
      return current + 1;
  }
  
  private boolean compareAndSet(int expected, int update) {
      return unsafe.compareAndSwapInt(this, counterOffset, expected, update); // Using Unsafe for illustration
  }
  ```

**3. Non-Blocking Data Structures:**

- **Concept:**  Data structures designed for concurrency without using locks, often relying on CAS operations and other
  lock-free techniques.
- **Examples:**
    - `ConcurrentLinkedQueue`: A lock-free, wait-free queue implementation.
    - `ConcurrentHashMap` (Java 8 and later):  Uses fine-grained locking for some operations but employs lock-free
      techniques for reads and updates on different segments of the hash table.

**4. Memory Barriers:**

- **Concept:** Instructions that enforce ordering constraints on memory operations, ensuring that writes made by one
  thread are visible to other threads in a timely and consistent manner.
- **Importance in Lock-Free Programming:**  Crucial for preventing subtle concurrency bugs that can arise from compiler
  optimizations or CPU memory reordering.
- **Java Support:**  The `volatile` keyword in Java implicitly enforces memory barriers.

**Advantages of Lock-Free Strategies:**

- **Increased Concurrency and Throughput:** Potential for higher performance, especially in highly concurrent systems,
  as threads don't block each other.
- **Deadlock Freedom:**  Lock-free algorithms are immune to deadlocks because they don't acquire locks.
- **Progress Guarantees:** Some lock-free algorithms offer stronger progress guarantees (e.g., wait-freedom,
  lock-freedom) than traditional locking.

**Disadvantages and Challenges:**

- **Complexity:**  Designing, implementing, and debugging lock-free algorithms is significantly more complex than
  traditional locking.
- **Liveness Issues:**  While lock-free algorithms avoid deadlocks, they can still suffer from liveness issues like
  starvation or livelocks in certain scenarios.
- **Platform Dependence:** The implementation and performance of lock-free techniques can be platform-dependent (CPU
  architecture, memory model).

**Key Takeaways:**

- Lock-free programming is a powerful but advanced technique.
- Consider using lock-free data structures provided by the Java standard library when appropriate.
- Carefully evaluate the trade-offs between performance, complexity, and the risk of introducing subtle concurrency bugs
  before implementing custom lock-free algorithms.

## Blocking Calls And Its Alternatives

In programming, a **blocking call** (also often called a **synchronous call**) is a type of function or method call *
*that pauses the execution of the calling thread until the called function completes its operation and returns a result.
**

**Key Characteristics of Blocking Calls:**

- **Synchronous Execution:** The calling thread waits for the called function to finish before proceeding.
- **Potential for Delays:** If the called function takes a long time to complete (e.g., a slow network request), the
  calling thread is blocked for that duration, potentially making your application unresponsive.

**Common Examples of Blocking Operations:**

- **Input/Output (I/O) Operations:** Reading from and writing to files, network sockets, databases.
- **Acquiring Resources:**  Waiting for a lock to become available, requesting access to a shared resource.
- **Some System Calls:** Certain system calls that interact with the operating system can be blocking.

**Alternatives to Blocking Calls:**

- **Asynchronous Calls (Non-Blocking):** The calling thread continues execution without waiting for the called function
  to complete. This is often handled using callbacks, promises, or async/await mechanisms.
- **Multithreading:** Creating additional threads allows you to perform blocking operations in the background without
  stalling the main thread.

### Thread-Per-Task Threading Model

- **Concept:** In this model, **you create a new thread** for each distinct task that your application needs to perform.
- **How it Works:**
    1. **Task Arrival:** When a new task arrives (e.g., a user request, a background job), you create a new thread.
    2. **Task Execution:**  The thread executes the logic associated with that task.
    3. **Thread Termination:**  Once the task is complete, the thread exits and is destroyed.

**Advantages of Thread-Per-Task:**

- **Simplicity:**  Straightforward to understand and implement, especially for independent tasks.

- **Isolation:** Each task runs in its own thread, providing a degree of isolation and preventing errors in one task
  from directly affecting others.

**Disadvantages and Considerations:**

- **Resource Consumption:** Creating and destroying threads can be resource-intensive, especially if you have a large
  number of short-lived tasks.
- **Thread Management Overhead:**  Managing a large number of threads can become complex, especially with
  synchronization and communication between threads.
- **Limited Scalability:**  Creating a thread for every task might not scale well for very high-volume systems, as
  thread creation and context switching overheads become significant.

**Alternatives to Thread-Per-Task:**

- **Thread Pools:** A more efficient approach where you create a pool of reusable worker threads that can handle
  multiple tasks. This reduces thread creation/destruction overhead.
- **Asynchronous Programming (Callbacks, Futures, Async/Await):**  **Allows you to perform tasks concurrently without
  directly managing threads.**
- **Actor Model:**  A higher-level concurrency model where independent "actors" communicate through messages, often used
  in distributed systems.

### Non-Blocking Calls

**Non-Blocking Calls** are operations that allow a thread to continue executing without waiting for the completion of a
task or operation. Instead of halting execution, the thread can perform other tasks or respond to events while waiting
for the operation to finish.

**Key Characteristics:**

1. **Asynchronous Execution**: Non-blocking calls are typically designed to work asynchronously, meaning they can
   initiate a task and return immediately, allowing the caller to continue processing.

2. **Callbacks or Promises**: Often, non-blocking calls use callbacks or promise-like constructs to handle the result of
   the operation once it completes. This allows the program to react when the data is ready without blocking.

3. **Event-Driven**: Many non-blocking operations are part of event-driven programming models, where events trigger
   actions based on the completion of tasks.

**Common Examples:**

- **I/O Operations**: Reading from or writing to files, network sockets, or databases where the operation can be
  initiated, and the program can continue running while waiting for the data.

- **Multithreading**: Using threads or worker pools to handle tasks in the background without blocking the main thread,
  allowing for responsive applications.

Advantages of Non-Blocking Calls:

- **Responsiveness**: Applications can remain responsive to user input and other events.
- **Resource Efficiency**: Non-blocking calls can lead to better resource utilization, as threads are not idly waiting
  for operations to complete.
- **Scalability**: They can handle a larger number of concurrent tasks without the overhead of managing multiple
  threads.

## Multhreading Techniques

**1. Thread Creation and Management**

- **Creating Threads:**
    - **Java:** Extend the `Thread` class or implement the `Runnable` interface, then create a `Thread` object and
      call `start()`.
    - **Python:** Use the `threading` module (`threading.Thread(target=my_function)`).
- **Thread Synchronization (Preventing Data Races):**
    - **Locks (Mutexes):** Ensure that only one thread can access a critical section of code at a time (
      e.g., `synchronized` blocks in Java, `threading.Lock` in Python).
    - **Semaphores:** Control access to a limited number of resources.
    - **Monitors (Implicit Locking):** Some languages (like Java) provide implicit locking mechanisms
      using `synchronized` methods.
- **Thread Communication:**
    - **Shared Memory:** Threads can communicate by reading and writing to shared data structures, but this requires
      careful synchronization.
    - **Wait and Notify (Condition Variables):**  Threads can signal each other based on specific conditions (
      e.g., `wait()` and `notify()` in Java).
    - **Message Passing (Channels, Queues):**  Threads communicate by sending messages through data structures like
      queues or channels.

**2. Thread Pooling**

- **Concept:** Create a pool of reusable worker threads to reduce the overhead of thread creation and destruction.
- **Benefits:**
    - Improved performance by reusing threads.
    - Control over the maximum number of concurrent threads.
- **Implementations:**
    - **Java:** `ExecutorService` and `ThreadPoolExecutor` classes.
    - **Python:** `concurrent.futures.ThreadPoolExecutor`.

**3. Asynchronous Programming**

- **Concept:** Allows a program to continue executing other tasks while waiting for long-running operations to complete,
  often without directly managing threads.
- **Techniques:**
    - **Callbacks:** Functions passed as arguments to be executed when an asynchronous operation finishes.
    - **Promises (Futures):** Objects representing the eventual result of an asynchronous operation.
    - **Async/Await (Modern Approach):**  Syntactic sugar that makes asynchronous code look more like synchronous code (
      available in many languages, including JavaScript, Python, C#).

**4. Lock-Free Data Structures and Algorithms**

- **Concept:**  Achieve thread safety without using traditional locking mechanisms, often relying on atomic operations (
  e.g., compare-and-swap).
- **Benefits:**  Potential for higher performance in highly concurrent scenarios.
- **Examples:**
    - **Concurrent Collections:**  `ConcurrentHashMap`, `ConcurrentLinkedQueue` (in Java).
    - **Atomic Variables:** `AtomicInteger`, `AtomicBoolean` (in Java).

**5. Functional Programming Techniques for Concurrency**

- **Immutability:** Using immutable data structures helps prevent data races, as data cannot be modified after creation.
- **Higher-Order Functions (Map, Reduce, Filter):**  Can be used to process data in parallel, especially with libraries
  designed for parallel execution (e.g., Java Streams).

**Choosing the Right Technique:**

- **Task Nature:** Are your tasks CPU-bound (computation-heavy) or I/O-bound (waiting for external operations)?
- **Concurrency Level:** How many threads or tasks will be running concurrently?
- **Complexity vs. Performance:**  Balance the complexity of the technique with the potential performance gains.
- **Language and Platform Support:**  Consider the features and libraries available in your programming language and
  platform.

### How Kotlin Coroutines Relate To Those Techniques:

**1. Not Quite Thread-Per-Task:**

- While you can launch a coroutine for each task, coroutines are **lightweight** and don't directly map 1:1 to native OS
  threads. Multiple coroutines can run concurrently on a smaller number of threads, managed by Kotlin's coroutine
  library.

**2. More Like Asynchronous Programming:**

- **Suspend and Resume:** Coroutines use `suspend` functions, which allow them to pause execution without blocking a
  thread. This is similar to the concept of asynchronous operations in callbacks, promises, or async/await.
- **Non-Blocking:** Coroutines promote non-blocking code, making applications more responsive, especially when dealing
  with I/O or long-running tasks.

**3. Leveraging Thread Pools (Behind the Scenes):**

- Kotlin coroutines typically execute on thread pools provided by dispatchers (
  e.g., `Dispatchers.IO`, `Dispatchers.Default`). These dispatchers manage a pool of threads, and coroutines can be
  dispatched to run on them.

**4. Abstraction Over Threads:**

- **Higher-Level Concurrency:**  Coroutines offer a higher-level abstraction over threads, making it easier to write
  asynchronous code that is more readable and maintainable. You focus on the logic of your operations rather than the
  low-level details of thread management.

**In Summary:** Kotlin coroutines provide a modern and efficient approach to concurrency that combines aspects of
asynchronous programming and thread pooling, while abstracting away much of the complexity of traditional thread
management.

**Key Advantages of Coroutines:**

- **Lightweight:** Coroutines have a lower memory footprint than traditional threads, allowing for a larger number of
  concurrent operations.
- **Readability:** Coroutine code often reads more like synchronous code, making it easier to understand and maintain.
- **Structured Concurrency:** Coroutines encourage structured concurrency patterns, reducing the risk of errors like
  resource leaks or unexpected behavior.

